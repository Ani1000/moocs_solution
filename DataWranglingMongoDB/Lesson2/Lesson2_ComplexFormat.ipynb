{
 "metadata": {
  "name": "",
  "signature": "sha256:796cb1ec702c5f4d763221f0b2a429e1aeb62191724a86ac65e89210f296c934"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %%writefile find_author_data.py\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# Your task here is to extract data from xml on authors of an article\n",
      "# and add it to a list, one item for an author.\n",
      "# See the provided data structure for the expected format.\n",
      "# The tags for first name, surname and email should map directly\n",
      "# to the dictionary keys\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "article_file = \"exampleResearchArticle.xml\"\n",
      "\n",
      "\n",
      "def get_root(fname):\n",
      "    tree = ET.parse(fname)\n",
      "    return tree.getroot()\n",
      "\n",
      "\n",
      "def get_authors(root):\n",
      "    authors = []\n",
      "    for author in root.findall('./fm/bibl/aug/au'):\n",
      "        data = {\n",
      "                \"fnm\": None,\n",
      "                \"snm\": None,\n",
      "                \"email\": None\n",
      "        }\n",
      "        \n",
      "\n",
      "        # YOUR CODE HERE\n",
      "        for key in data:\n",
      "            data[key] = author.find(key).text\n",
      "            \n",
      "        data['insr'] = [e.attrib['iid'] for e in author.findall('insr')]\n",
      "        authors.append(data)\n",
      "\n",
      "    return authors\n",
      "\n",
      "\n",
      "def test():\n",
      "    #withour insr\n",
      "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
      "    \n",
      "    root = get_root(article_file)\n",
      "    data = get_authors(root)\n",
      "    assert data[0] == solution[0]\n",
      "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
      "    \n",
      "    \n",
      "    #with insr\n",
      "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
      "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
      "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
      "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
      "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
      "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
      "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
      "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
      "\n",
      "    root = get_root(article_file)\n",
      "    data = get_authors(root)\n",
      "\n",
      "    assert data[0] == solution[0]\n",
      "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
      "\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting find_author_data.py\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %%writefile post_html_form.py\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# Please note that the function 'make_request' is provided for your reference only.\n",
      "# You will not be able to to actually use it from within the Udacity web UI.\n",
      "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
      "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the approprate\n",
      "# values in the data dictionary.\n",
      "# All your changes should be in the 'extract_data' function\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import json\n",
      "\n",
      "html_page = \"page_source.html\"\n",
      "\n",
      "\n",
      "def extract_data(page):\n",
      "    data = {\"eventvalidation\": \"\",\n",
      "            \"viewstate\": \"\"}\n",
      "    with open(page, \"r\") as html:\n",
      "        soup = BeautifulSoup(html)\n",
      "        data['eventvalidation'] = soup.find(id='__EVENTVALIDATION')['value']\n",
      "        data['viewstate']  = soup.find(id='__VIEWSTATE')['value']\n",
      "        \n",
      "        \n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def make_request(data):\n",
      "    eventvalidation = data[\"eventvalidation\"]\n",
      "    viewstate = data[\"viewstate\"]\n",
      "\n",
      "    #for managed session, use s instead of r\n",
      "#     s = requests.Session()\n",
      "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
      "                    data={'AirportList': \"BOS\",\n",
      "                          'CarrierList': \"VX\",\n",
      "                          'Submit': 'Submit',\n",
      "                          \"__EVENTTARGET\": \"\",\n",
      "                          \"__EVENTARGUMENT\": \"\",\n",
      "                          \"__EVENTVALIDATION\": eventvalidation,\n",
      "                          \"__VIEWSTATE\": viewstate\n",
      "                    })\n",
      "\n",
      "    return r.text\n",
      "\n",
      "\n",
      "def test():\n",
      "    data = extract_data(html_page)\n",
      "    assert data[\"eventvalidation\"] != \"\"\n",
      "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
      "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
      "\n",
      "    \n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile process_all.py\n",
      "\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# Let's assume that you combined the code from the previous 2 exercises\n",
      "# with code from the lesson on how to build requests, and downloaded all the data locally.\n",
      "# The files are in a directory \"data\", named after the carrier and airport:\n",
      "# \"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
      "# The table with flight info has a table class=\"dataTDRight\".\n",
      "# There are couple of helper functions to deal with the data files.\n",
      "# Please do not change them for grading purposes.\n",
      "# All your changes should be in the 'process_file' function\n",
      "# This is example of the datastructure you should return\n",
      "# Each item in the list should be a dictionary containing all the relevant data\n",
      "# Note - year, month, and the flight data should be integers\n",
      "# You should skip the rows that contain the TOTAL data for a year\n",
      "# data = [{\"courier\": \"FL\",\n",
      "#         \"airport\": \"ATL\",\n",
      "#         \"year\": 2012,\n",
      "#         \"month\": 12,\n",
      "#         \"flights\": {\"domestic\": 100,\n",
      "#                     \"international\": 100}\n",
      "#         },\n",
      "#         {\"courier\": \"...\"}\n",
      "# ]\n",
      "from bs4 import BeautifulSoup\n",
      "from zipfile import ZipFile\n",
      "import os\n",
      "\n",
      "datadir = \"data\"\n",
      "\n",
      "\n",
      "def open_zip(datadir):\n",
      "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def process_all(datadir):\n",
      "    files = os.listdir(datadir)\n",
      "    return files\n",
      "\n",
      "\n",
      "def process_file(f):\n",
      "    # This is example of the datastructure you should return\n",
      "    # Each item in the list should be a dictionary containing all the relevant data\n",
      "    # Note - year, month, and the flight data should be integers\n",
      "    # You should skip the rows that contain the TOTAL data for a year\n",
      "    # data = [{\"courier\": \"FL\",\n",
      "    #         \"airport\": \"ATL\",\n",
      "    #         \"year\": 2012,\n",
      "    #         \"month\": 12,\n",
      "    #         \"flights\": {\"domestic\": 100,\n",
      "    #                     \"international\": 100}\n",
      "    #         },\n",
      "    #         {\"courier\": \"...\"}\n",
      "    # ]\n",
      "    data = []\n",
      "    info = {}\n",
      "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
      "    #print info\n",
      "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
      "        soup = BeautifulSoup(html)\n",
      "        table = soup.find('table', {'class':'dataTDRight'})\n",
      "        for row in table.findAll('tr')[1:]:\n",
      "            fields = row.findAll('td')\n",
      "            fields = [e.text.replace(',','') for e in fields]\n",
      "            try:\n",
      "                fields.index('TOTAL')\n",
      "            except ValueError:\n",
      "                fields = [int(float(e)) for e in fields ]\n",
      "                info['year'] = fields[0]\n",
      "                info['month'] = fields[1]\n",
      "                info['flights'] = {\n",
      "                                'domestic':fields[2],\n",
      "                                'international':fields[3],\n",
      "                                }\n",
      "                data.append(info)\n",
      "                \n",
      "                \n",
      "        \n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    print \"Running a simple test...\"\n",
      "    open_zip(datadir)\n",
      "    files = process_all(datadir)\n",
      "    data = []\n",
      "    for f in files:\n",
      "        data += process_file(f)\n",
      "    #print data\n",
      "    assert len(data) == 399\n",
      "    for entry in data[:3]:\n",
      "        assert type(entry[\"year\"]) == int\n",
      "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
      "        assert len(entry[\"airport\"]) == 3\n",
      "        assert len(entry[\"courier\"]) == 2\n",
      "    assert data[-1][\"airport\"] == \"ATL\"\n",
      "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
      "    \n",
      "    print \"... success!\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing process_all.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}